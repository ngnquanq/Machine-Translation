{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogUDqQOxSpgG",
        "outputId": "087bf0d9-ab75-48d7-9fa3-45970b3c5689"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcXbu9X_QaF2",
        "outputId": "f672af5c-b307-434d-9e77-b88abf6d60bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: underthesea in /usr/local/lib/python3.10/dist-packages (6.8.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.7)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.10/dist-packages (from underthesea) (0.9.10)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.31.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.3.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0.1)\n",
            "Requirement already satisfied: underthesea-core==1.0.4 in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.0.4)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2023.12.25)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2024.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install underthesea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h95fkSiWQaF4",
        "outputId": "39af3305-9f5a-480f-834c-db68dafad3a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.8.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.12.25)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.25.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yf1ckijRT2j",
        "outputId": "05e07bd9-3ce7-41f6-9211-33a32709c8be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNG5Zo_-QaF4",
        "outputId": "1ddc2f1b-91f5-48a6-c633-c39506c15d41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_flax_utils.py:24: FutureWarning: Importing `FlaxGenerationMixin` from `src/transformers/generation_flax_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import FlaxGenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import sacrebleu\n",
        "import underthesea\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iWmOgK1BQaF5"
      },
      "outputs": [],
      "source": [
        "# Create dataset\n",
        "class NMTDataset(Dataset):\n",
        "    def __init__(self, cfg, data_type = 'train'):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.src_texts, self.tgt_text = self.read_data(data_type)\n",
        "        self.src_input_ids, self.src_attention_mask = self.texts_to_sequence(self.src_texts)\n",
        "        self.tgt_input_ids, self.tgt_attention_mask, self.labels = self.texts_to_sequence(self.tgt_text,\n",
        "                                                                                          is_src = False)\n",
        "\n",
        "    def read_data(self, data_type):\n",
        "        data = load_dataset(\"mt_eng_vietnamese\", \"iwslt2015-en-vi\", split = data_type)\n",
        "        src_texts = [sample['translation'][self.cfg.src_lang] for sample in data]\n",
        "        tgt_texts = [sample['translation'][self.cfg.tgt_lang] for sample in data]\n",
        "        return src_texts, tgt_texts\n",
        "\n",
        "    def texts_to_sequence(self, texts, is_src = True):\n",
        "        if is_src:\n",
        "            src_inputs = self.cfg.src_tokenizer(texts, max_length = self.cfg.src_max_len,\n",
        "                                                padding = 'max_length', return_tensors = 'pt',\n",
        "                                                truncation = True)\n",
        "            return (\n",
        "                src_inputs.input_ids,\n",
        "                src_inputs.attention_mask\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            if self.cfg.add_special_tokens == True:\n",
        "                texts = [\n",
        "                    ' '.join(\n",
        "                        [self.cfg.tgt_tokenizer.bos_token, underthesea.word_tokenize(text), self.cfg.tgt_tokenizer.eos_token]\n",
        "                    ) for text in texts\n",
        "                ]\n",
        "            tgt_inputs = self.cfg.tgt_tokenizer(texts, padding = 'max_length', truncation = True,\n",
        "                                                max_length = self.cfg.tgt_max_len, return_tensors = 'pt')\n",
        "            labels  = tgt_inputs.input_ids.numpy().tolist()\n",
        "            labels = [\n",
        "                [\n",
        "                    -100 if token_id == self.cfg.tgt_tokenizer.pad_token_id else token_id for token_id in label\n",
        "                ]\n",
        "                for label in labels\n",
        "            ]\n",
        "            labels = torch.LongTensor(labels)\n",
        "\n",
        "            return (\n",
        "                tgt_inputs.input_ids,\n",
        "                tgt_inputs.attention_mask,\n",
        "                labels\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return{\n",
        "            \"input_ids\": self.src_input_ids[index],\n",
        "            \"attention_mask\": self.src_attention_mask[index],\n",
        "            \"labels\": self.labels[index],\n",
        "            \"decoder_input_ids\": self.tgt_input_ids[index],\n",
        "            \"decoder_attention_mask\": self.tgt_attention_mask[index]\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return np.shape(self.src_input_ids)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BqiEslKKQaF6"
      },
      "outputs": [],
      "source": [
        "# TOkenizer\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [label.strip() for label in labels]\n",
        "\n",
        "    return preds, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6oGSm8M2QaF6"
      },
      "outputs": [],
      "source": [
        "def load_tokenizer(model_name_or_path):\n",
        "    if 'bert' in model_name_or_path.split('-'):\n",
        "        return BertTokenizerFast.from_pretrained(model_name_or_path)\n",
        "    elif 'gpt2' in model_name_or_path.split('-'):\n",
        "        return GPT2TokenizerFast.from_pretrained(model_name_or_path)\n",
        "    else:\n",
        "        return AutoTokenizer.from_pretrained(model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "H1pv5F9PQaF6"
      },
      "outputs": [],
      "source": [
        "# Trainer class\n",
        "class Manager():\n",
        "    def __init__(self, cfg, is_train = True):\n",
        "        self.cfg = cfg\n",
        "\n",
        "        print('Loading tokenizer...')\n",
        "        self.get_tokenizer()\n",
        "\n",
        "        print('Loading model...')\n",
        "        self.get_model()\n",
        "\n",
        "        print('Loading metric...')\n",
        "        self.bleu_metric = load_metric('sacrebleu')\n",
        "\n",
        "        print('Check save model path')\n",
        "        if not os.path.exists(self.cfg.ckpt_dir):\n",
        "            os.mkdir(self.cfg.ckpt_dir)\n",
        "\n",
        "        if is_train:\n",
        "            # Load dataset\n",
        "            print('Loading dataset...')\n",
        "            self.train_dataset = NMTDataset(self.cfg, 'train')\n",
        "            self.valid_dataset = NMTDataset(self.cfg, 'validation')\n",
        "\n",
        "        print(\"Setting finished\")\n",
        "\n",
        "    def get_tokenizer(self):\n",
        "        if self.cfg.load_model_from_path:\n",
        "            self.cfg.src_tokenizer = load_tokenizer(self.cfg.ckpt_dir)\n",
        "            self.cfg.tgt_tokenizer = load_tokenizer(self.cfg.ckpt_dir)\n",
        "        else:\n",
        "            self.cfg.src_tokenizer = load_tokenizer(self.cfg.src_model_name)\n",
        "            self.cfg.tgt_tokenizer = load_tokenizer(self.cfg.tgt_model_name)\n",
        "            if \"bert\" in self.cfg.tgt_model_name.split(\"-\"):\n",
        "                self.cfg.add_special_tokens = False\n",
        "                self.cfg.bos_token_id = self.cfg.tgt_tokenizer.cls_token_id\n",
        "                self.cfg.eos_token_id = self.cfg.tgt_tokenizer.sep_token_id\n",
        "                self.cfg.pad_token_id = self.cfg.tgt_tokenizer.pad_token_id\n",
        "            else:\n",
        "                self.cfg.add_special_tokens = True\n",
        "                self.cfg.tgt_tokenizer.add_special_tokens(\n",
        "                    {'bos_token': '[BOS]',\n",
        "                     'eos_token': '[EOS]',\n",
        "                        'pad_token': '[PAD]'})\n",
        "                self.cfg.bos_token_id = self.cfg.tgt_tokenizer.bos_token_id\n",
        "                self.cfg.eos_token_id = self.cfg.tgt_tokenizer.eos_token_id\n",
        "                self.cfg.pad_token_id = self.cfg.tgt_tokenizer.pad_token_id\n",
        "                self.cfg.src_tokenizer.save_pretrained(os.path.join(self.cfg.ckpt_dir, f'{self.cfg.src_lang}_tokenizer_{self.cfg.src_model_name}'))\n",
        "                self.cfg.tgt_tokenizer.save_pretrained(os.path.join(self.cfg.ckpt_dir, f'{self.cfg.tgt_lang}_tokenizer_{self.cfg.tgt_model_name}'))\n",
        "\n",
        "    def get_model(self):\n",
        "        if self.cfg.load_model_from_path:\n",
        "            save_model_path = os.path.join(self.cfg.ckpt_dir, self.cfg.ckpt_name)\n",
        "            self.model = EncoderDecoderModel.from_pretrained(save_model_path)\n",
        "        else:\n",
        "            self.model = EncoderDecoderModel.from_encoder_decoder_pretrained(self.cfg.src_model_name,\n",
        "                                                                            self.cfg.tgt_model_name)\n",
        "            self.model.decoder.resize_token_embeddings(len(self.cfg.tgt_tokenizer))\n",
        "            self.model.config.decoder_start_token_id = self.cfg.bos_token_id\n",
        "            self.model.config.eos_token_id = self.cfg.eos_token_id\n",
        "            self.model.config.pad_token_id = self.cfg.pad_token_id\n",
        "            self.model.config.vocab_size = len(self.cfg.tgt_tokenizer)\n",
        "            self.model.config.max_length = self.cfg.max_length_decoder\n",
        "            self.model.config.min_length = self.cfg.min_length_decoder\n",
        "            self.model.config.no_repeat_ngram_size = 3\n",
        "            self.model.config.early_stopping = True\n",
        "            self.model.config.length_penalty = 2.0\n",
        "            self.model.config.num_beams = self.cfg.beam_size\n",
        "\n",
        "    def train(self):\n",
        "        print('Start training...')\n",
        "        if self.cfg.use_eval_steps:\n",
        "            training_args = Seq2SeqTrainingArguments(\n",
        "                predict_with_generate=True,\n",
        "                evaluation_strategy='steps',\n",
        "                save_strategy='steps',\n",
        "                save_steps=self.cfg_eval_steps,\n",
        "                eval_steps=self.cfg.eval_steps,\n",
        "                output_dir=self.cfg.ckpt_dir,\n",
        "                per_device_train_batch_size=self.cfg.train_batch_size,\n",
        "                per_device_eval_batch_size=self.cfg.eval_batch_size,\n",
        "                learning_rate = self.cfg.learning_rate,\n",
        "                weight_decay=5e-3,\n",
        "                num_train_epochs=self.cfg.num_train_epochs)\n",
        "        else:\n",
        "            training_args = Seq2SeqTrainingArguments(\n",
        "                predict_with_generate=True,\n",
        "                evaluation_strategy='epoch',\n",
        "                save_strategy='epoch',\n",
        "                output_dir=self.cfg.ckpt_dir,\n",
        "                per_device_train_batch_size=self.cfg.train_batch_size,\n",
        "                per_device_eval_batch_size=self.cfg.eval_batch_size,\n",
        "                learning_rate=self.cfg.learning_rate,\n",
        "                weight_decay=5e-3,\n",
        "                num_train_epochs=self.cfg.num_train_epochs)\n",
        "\n",
        "        data_collator = DataCollatorForSeq2Seq(tokenizer=self.cfg.tgt_tokenizer, model=self.model)\n",
        "\n",
        "        trainer = Seq2SeqTrainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            data_collator=data_collator,\n",
        "            train_dataset=self.train_dataset,\n",
        "            eval_dataset=self.valid_dataset,\n",
        "            tokenizer=self.cfg.tgt_tokenizer,\n",
        "            compute_metrics=self.compute_metrics\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "    def compute_metrics(self, eval_preds):\n",
        "        preds, labels = eval_preds\n",
        "        if isinstance(preds, tuple):\n",
        "            preds = preds[0]\n",
        "        decoded_preds = self.cfg.tgt_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "        labels = np.where(labels != -100, labels, self.cfg.tgt_tokenizer.pad_token_id)\n",
        "        decoded_labels = self.cfg.tgt_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "        result = self.bleu_metric.compute(predictions = decoded_preds, references = [decoded_labels])\n",
        "        result = {'bleu': result['score']}\n",
        "\n",
        "        prediciton_lens = [np.count_nonzero(pred != self.cfg.tgt_tokenizer.pad_token_id) for pred in preds]\n",
        "        result['gen_len'] = np.mean(prediciton_lens)\n",
        "        result = {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zkIX950cQaF6",
        "outputId": "94a6de9b-7a59-40ce-fb16-3ad954e9e04f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/vocab.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.37.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/vocab.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.37.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.37.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.37.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "Initializing bert-base-multilingual-cased as a decoder model. Cross attention layers are added to bert-base-multilingual-cased and randomly initialized if bert-base-multilingual-cased's architecture allows for cross attention layers.\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/model.safetensors\n",
            "Generate config GenerationConfig {\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertLMHeadModel: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Generation config file not found, using a generation config created from the model config.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "Generate config GenerationConfig {}\n",
            "\n",
            "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 119547. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
            "<ipython-input-9-285861d99de7>:13: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  self.bleu_metric = load_metric('sacrebleu')\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/sacrebleu/sacrebleu.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading metric...\n",
            "Check save model path\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting finished\n",
            "Start training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 133,318\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 83,330\n",
            "  Number of trainable parameters = 384,194,811\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:640: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='83330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [    2/83330 : < :, Epoch 0.00/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Config\n",
        "class BaseConfig:\n",
        "    def __init__(self,**kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "# NMTConfig\n",
        "class NMTConfig(BaseConfig):\n",
        "    src_lang = 'en'\n",
        "    tgt_lang = 'vi'\n",
        "    src_max_len = 75\n",
        "    tgt_max_len = 75\n",
        "\n",
        "    #mdoel\n",
        "    src_model_name = \"bert-base-multilingual-cased\"\n",
        "    tgt_model_name = \"bert-base-multilingual-cased\"\n",
        "\n",
        "    #Training\n",
        "    load_model_from_path = False\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    learning_rate = 3e-5\n",
        "    train_batch_size = 16\n",
        "    eval_batch_size = 16\n",
        "    num_train_epochs = 10\n",
        "    ckpt_dir = src_model_name + '_to_' + tgt_model_name\n",
        "    use_eval_steps = False\n",
        "    eval_steps = 400\n",
        "\n",
        "    #Inference\n",
        "    max_length_decoder = 75\n",
        "    min_length_decoder = 25\n",
        "    beam_size = 1\n",
        "\n",
        "cfg = NMTConfig()\n",
        "manager = Manager(cfg, is_train=True)\n",
        "manager.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGut-sGSQaF7"
      },
      "outputs": [],
      "source": [
        "#prediction\n",
        "def load_model(cfg, checkpoint_name):\n",
        "    # Load Tokenizer\n",
        "    src_tokenizer_save_path = f\"{cfg.ckpt_dir}/{cfg.src_lang}_tokenizer_{cfg.src_model_name}\"\n",
        "    src_tokenizer = BertTokenizerFast.from_pretrained(src_tokenizer_save_path)\n",
        "\n",
        "    tgt_tokenizer_save_path = f\"{cfg.ckpt_dir}/{cfg.tgt_lang}_tokenizer_{cfg.tgt_model_name}\"\n",
        "    tgt_tokenizer = GPT2TokenizerFast.from_pretrained(tgt_tokenizer_save_path)\n",
        "\n",
        "    # Load Model\n",
        "    model_save_path = f\"{cfg.ckpt_dir}/{checkpoint_name}\"\n",
        "    model = EncoderDecoderModel.from_pretrained(model_save_path)\n",
        "\n",
        "    # Inference Param\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    return src_tokenizer, tgt_tokenizer, model, device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXmtY9EoQaF8"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def inference(text, src_tokenizer, tgt_tokenizer, model, device=\"cpu\", max_length=75, beam_size=5):\n",
        "    inputs = src_tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    input_ids = inputs.input_ids.to(device)\n",
        "    attention_mask = inputs.attention_mask.to(device)\n",
        "    model.to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=max_length,\n",
        "        early_stopping=True,\n",
        "        num_beams=beam_size,\n",
        "        length_penalty=2.0\n",
        "    )\n",
        "\n",
        "    output_str = tgt_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    return output_str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QQ1-RAtQaF9"
      },
      "outputs": [],
      "source": [
        "def inference_batch(texts, src_tokenizer, tgt_tokenizer, model, device=\"cpu\", max_length=75, beam_size=5, batch_size=32):\n",
        "\n",
        "    pred_texts = []\n",
        "\n",
        "    if len(texts) < batch_size:\n",
        "        batch_size = len(texts)\n",
        "\n",
        "    for x in tqdm(range(0, len(texts), batch_size)):\n",
        "        text = texts[x:x+batch_size]\n",
        "\n",
        "        inputs = src_tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = inputs.input_ids.to(device)\n",
        "        attention_mask = inputs.attention_mask.to(device)\n",
        "        model.to(device)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=max_length,\n",
        "            early_stopping=True,\n",
        "            num_beams=beam_size,\n",
        "            length_penalty=2.0\n",
        "        )\n",
        "\n",
        "        output_str = tgt_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        pred_texts.extend(output_str)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return pred_texts\n",
        "\n",
        "class BaseConfig:\n",
        "    \"\"\"Base Encoder Decoder config\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "class NMTConfig(BaseConfig):\n",
        "    # Data\n",
        "    src_lang = 'en'\n",
        "    tgt_lang = 'vi'\n",
        "    src_max_len = 75\n",
        "    tgt_max_len = 75\n",
        "\n",
        "    # Model\n",
        "    src_model_name = \"bert-base-multilingual-cased\"\n",
        "    tgt_model_name = \"bert-base-multilingual-cased\"\n",
        "\n",
        "    # Training\n",
        "    load_model_from_path = False\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    learning_rate = 3e-5\n",
        "    train_batch_size = 16\n",
        "    eval_batch_size = 8\n",
        "    num_train_epochs = 15\n",
        "    ckpt_dir = src_model_name + '_to_' + tgt_model_name\n",
        "    use_eval_steps = False\n",
        "    eval_steps = 2000\n",
        "\n",
        "    # Inference\n",
        "    max_length_decoder = 75\n",
        "    min_length_decoder = 25\n",
        "    beam_size = 5\n",
        "\n",
        "cfg = NMTConfig()\n",
        "\n",
        "# load data\n",
        "data = load_dataset(\"mt_eng_vietnamese\", \"iwslt2015-en-vi\", split=\"test\")\n",
        "src_texts = [sample[\"translation\"][\"en\"] for sample in data]\n",
        "\n",
        "tgt_texts = [sample[\"translation\"][\"vi\"] for sample in data]\n",
        "\n",
        "src_tokenizer, tgt_tokenizer, model, device = load_model(cfg, checkpoint_name=\"checkpoint-41665\")\n",
        "\n",
        "pred_texts = inference_batch(src_texts, src_tokenizer, tgt_tokenizer, model, device, beam_size=1)\n",
        "\n",
        "sacrebleu.corpus_bleu(pred_texts, [tgt_texts])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7MSqkE9fRNj5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}