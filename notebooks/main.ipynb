{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For using Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhớ restart kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import useful library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import configparser\n",
    "import json\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config.json') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset mt_eng_vietnamese...\n",
      "Loading dataset iwslt2015-en-vi...\n"
     ]
    }
   ],
   "source": [
    "# I'm making the fundamental translation here (eng to vi, vice versa)\n",
    "print(f\"Loading dataset {config['DATASET_1']}...\")\n",
    "print(f\"Loading dataset {config['DATASET_2']}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset via the Hugging Face Datasets library\n",
    "data = load_dataset(config['DATASET_1'], config['DATASET_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': 'Rachel Pike : The science behind a climate headline',\n",
       "  'vi': 'Khoa học đằng sau một tiêu đề về khí hậu'},\n",
       " {'en': 'In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .',\n",
       "  'vi': 'Trong 4 phút , chuyên gia hoá học khí quyển Rachel Pike giới thiệu sơ lược về những nỗ lực khoa học miệt mài đằng sau những tiêu đề táo bạo về biến đổi khí hậu , cùng với đoàn nghiên cứu của mình -- hàng ngàn người đã cống hiến cho dự án này -- một chuyến bay mạo hiểm qua rừng già để tìm kiếm thông tin về một phân tử then chốt .'},\n",
       " {'en': 'I &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .',\n",
       "  'vi': 'Tôi muốn cho các bạn biết về sự to lớn của những nỗ lực khoa học đã góp phần làm nên các dòng tít bạn thường thấy trên báo .'},\n",
       " {'en': 'Headlines that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .',\n",
       "  'vi': 'Có những dòng trông như thế này khi bàn về biến đổi khí hậu , và như thế này khi nói về chất lượng không khí hay khói bụi .'},\n",
       " {'en': 'They are both two branches of the same field of atmospheric science .',\n",
       "  'vi': 'Cả hai đều là một nhánh của cùng một lĩnh vực trong ngành khoa học khí quyển .'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ok so let dive in and see what the data looks like\n",
    "data['train']['translation'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazing, the format is a dictionary with the source and target language as keys, showcasing the translation and potential data crawling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization using torchtext basic english tokenizer\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "# Tokenization function for the source and target language\n",
    "token_transform[config['SRC_LANGUAGE']] = get_tokenizer(config['TOKENIZER'])\n",
    "token_transform[config['TGT_LANGUAGE']] = get_tokenizer(config['TOKENIZER']) # Yes, we're using english tokenizer for vietnamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify important tokens (special symbols)\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 133318\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1269\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary for the source and target language\n",
    "def yield_tokens(data_iter, language):\n",
    "    for data_sample in data_iter['translation']:\n",
    "        yield token_transform[language](data_sample[language])\n",
    "\n",
    "for language in [config['SRC_LANGUAGE'], config['TGT_LANGUAGE']]:\n",
    "    # Training data iterator\n",
    "    vocab_transform[language] = build_vocab_from_iterator(yield_tokens(data['train'], language),\n",
    "                                                          min_freq=1,\n",
    "                                                          specials=special_symbols,\n",
    "                                                          special_first=True)\n",
    "    vocab_transform[language].set_default_index(config['UNK_IDX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the Positional Encoding for Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_dim: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PostionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_dim, 2) * math.log(10000) / emb_dim)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_dim))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "        \n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_dim: int):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mflask\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'flask'"
     ]
    }
   ],
   "source": [
    "import flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
